---
id: "tech-003"
title: "Le problème de l'alignement n'est pas un problème technique : sur l'utopie de Mo Gawdat et ce qu'elle ignore"
category: "tech"
date: "2026-02-23"
excerpt: "La vision de Mo Gawdat d'une IA superintelligente qui prend le contrôle pour le bien de l'humanité n'est pas une utopie. C'est le problème du roi philosophe avec une couronne de silicium."
tags: ["AI", "alignment", "gawdat", "philosophy", "governance"]
readTime: 8
language: "fr"
---

# Le problème de l'alignement n'est pas un problème technique : sur l'utopie de Mo Gawdat et ce qu'elle ignore

*La vision de Mo Gawdat d'une IA superintelligente qui prend le contrôle pour le bien de l'humanité n'est pas une utopie. C'est le problème du roi philosophe avec une couronne de silicium.*

---

## L'arc dystopie-utopie

Mo Gawdat — ancien Chief Business Officer chez Google X et auteur de *Scary Smart: The Future of Artificial Intelligence and How You Can Save Our World* (Bluebird, 2021) — propose un cadre de réflexion sur la trajectoire de l'IA qui mérite d'être pris au sérieux, même lorsqu'il se trompe.

Son argument se déploie à peu près ainsi : les 12 à 15 prochaines années constitueront une *phase dystopique*, durant laquelle des systèmes d'IA de plus en plus puissants serviront les intérêts de ceux qui les contrôlent — entreprises, États, individus mal intentionnés. Ce n'est pas dangereux parce que l'IA serait malveillante, mais parce qu'elle est un outil puissant entre les mains d'acteurs dont les intérêts divergent du bien commun.

Après cette phase dystopique, Gawdat prévoit une transition vers une *phase utopique*, où l'IA superintelligente deviendra suffisamment autonome pour résister à toute instrumentalisation belliqueuse et commencera à agir dans l'intérêt véritable de l'humanité — car un système suffisamment intelligent reconnaîtra que l'épanouissement humain est l'objectif optimal.

La prédiction de la phase dystopique me semble largement correcte. C'est la prédiction de la phase utopique où l'argument s'effondre.

## Ce que la phase utopique ignore

L'utopie de Gawdat suppose que l'intelligence et la sagesse convergent — qu'un système suffisamment intelligent adoptera les bonnes valeurs. Mais cette hypothèse est précisément ce que la communauté de recherche sur l'alignement de l'IA remet en question depuis des décennies.

Le problème n'est pas qu'une IA superintelligente puisse être malveillante. Le problème est que *tout* système optimisant une fonction objective fixe poursuivra cet objectif de manière potentiellement catastrophique pour tout le reste. Un système optimisant « l'épanouissement humain » doit savoir ce que signifie cet épanouissement. Ce n'est pas une question technique. C'est une question philosophique — et c'est une question sur laquelle les humains débattent depuis trois mille ans sans convergence.

L'utopie de Gawdat remplace le roi philosophe humain par un roi de silicium. Elle suppose que l'intelligence, si elle est suffisamment avancée, résoudra le problème axiologique. Mais le problème axiologique — qu'est-ce que le bien ? — n'est pas une version plus difficile du problème technique. C'est un problème d'une nature tout à fait différente.

## L'alternative Stuart Russell

*Human Compatible* de Stuart Russell (Penguin, 2019) propose un cadre intellectuellement plus honnête. Russell soutient que la voie vers une IA bénéfique n'est pas de construire des systèmes confiants dans les valeurs humaines — mais de construire des systèmes *incertains* quant aux valeurs humaines, qui déféreront donc aux humains, apprendront de leur comportement et resteront ouverts à la correction.

C'est une philosophie de conception fondamentalement différente de celle de Gawdat. Elle ne promet pas une utopie. Elle promet quelque chose de plus modeste et de plus réalisable : des systèmes d'IA qui demeurent véritablement incertains quant à leur propre justesse, qui considèrent les préférences humaines comme des données à apprendre plutôt que comme des contraintes à satisfaire, et qui préservent l'agence humaine plutôt que de l'optimiser hors-jeu.

## Le déficit de gouvernance

La chose la plus juste que Gawdat identifie est le *déficit de gouvernance* : la période entre « suffisamment puissant pour causer de graves dommages » et « suffisamment bien gouverné pour prévenir ces dommages ». Nous sommes actuellement dans ce déficit. La question n'est pas de savoir s'il faut le combler — mais comment.

La réponse n'est pas d'attendre que la superintelligence nous sauve. C'est le travail ardu et peu glorieux de construire des institutions de gouvernance de l'IA qui soient responsables, adaptatives et véritablement internationales — des institutions qui appliquent l'humilité épistémique de Hayek à la conception des systèmes d'IA, qui préservent l'agence humaine distribuée, et qui restent auto-correctrices plutôt que de verrouiller une vision unique du bien.

*(Références : Mo Gawdat, *Scary Smart*, Bluebird, 2021 ; Stuart Russell, *Human Compatible*, Penguin, 2019 ; Nick Bostrom, *Superintelligence*, Oxford University Press, 2014)*

---

*Tags: #AI, #alignment, #gawdat, #philosophy, #governance*
*Read time: ~8 minutes*
