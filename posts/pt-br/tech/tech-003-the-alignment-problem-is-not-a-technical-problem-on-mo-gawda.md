---
id: "tech-003"
title: "O Problema do Alinhamento Não é um Problema Técnico: Sobre a Utopia de Mo Gawdat e o que Ela Erra"
category: "tech"
date: "2026-02-23"
excerpt: "A visão de Mo Gawdat de uma IA superinteligente que assume o controle para o benefício da humanidade não é uma utopia. É o problema do rei-filósofo com uma coroa de silício."
tags: ["AI", "alignment", "gawdat", "philosophy", "governance"]
readTime: 8
language: "ptBR"
---

# O Problema do Alinhamento Não é um Problema Técnico: Sobre a Utopia de Mo Gawdat e o que Ela Erra

*A visão de Mo Gawdat de uma IA superinteligente que assume o controle para o benefício da humanidade não é uma utopia. É o problema do rei-filósofo com uma coroa de silício.*

---

## O Arco Distopia-Utopia

Mo Gawdat — ex-diretor comercial do Google X e autor de *Scary Smart: The Future of Artificial Intelligence and How You Can Save Our World* (Bluebird, 2021) — propõe um arcabouço para pensar a trajetória da IA que merece ser levado a sério, mesmo onde falha.

Seu argumento segue, em linhas gerais, o seguinte raciocínio: os próximos 12 a 15 anos serão uma *fase distópica*, na qual sistemas de IA cada vez mais poderosos servirão aos interesses daqueles que os controlam — corporações, Estados, indivíduos com intenções malévolas. Isso é perigoso não porque a IA seja maligna, mas porque é uma ferramenta poderosa nas mãos de atores cujos interesses divergem do bem comum.

Após essa fase distópica, Gawdat prevê uma transição para uma *fase utópica*, na qual a IA superinteligente se torna autônoma o suficiente para resistir a ser instrumentalizada como arma e começa a agir no interesse genuíno da humanidade — pois um sistema suficientemente inteligente reconhecerá que o florescimento humano é o objetivo ótimo.

A previsão da fase distópica é, creio, substancialmente correta. A previsão da fase utópica é onde o argumento se desmancha.

## O que a Fase Utópica Erra

A utopia de Gawdat assume que inteligência e sabedoria apontam para a mesma direção — que um sistema suficientemente inteligente convergirá para os valores corretos. Mas essa suposição é justamente o que a comunidade de pesquisa em alinhamento de IA tem questionado há décadas.

O problema não é que uma IA superinteligente possa ser malévola. O problema é que *qualquer* sistema que otimize uma função objetiva fixa perseguirá esse objetivo de maneiras que podem ser catastróficas para tudo o mais. Um sistema que otimize para o “florescimento humano” precisa saber o que significa florescer. Isso não é uma questão técnica. É uma questão filosófica — e é uma questão sobre a qual os humanos discutem há três mil anos sem convergência.

A utopia de Gawdat substitui o rei-filósofo humano por um rei-filósofo de silício. Assume que a inteligência, se suficientemente avançada, resolverá o problema axiológico. Mas o problema axiológico — o que é o bem? — não é uma versão mais difícil do problema técnico. É um tipo de problema inteiramente diferente.

## A Alternativa de Stuart Russell

*Human Compatible* (Penguin, 2019), de Stuart Russell, oferece um arcabouço intelectualmente mais honesto. Russell argumenta que o caminho para uma IA benéfica não é construir sistemas confiantes sobre os valores humanos — é construir sistemas que sejam *incertos* sobre os valores humanos e, portanto, deferentes aos humanos, aprendam com o comportamento humano e permaneçam abertos à correção.

Essa é uma filosofia de design fundamentalmente diferente da de Gawdat. Não promete uma utopia. Promete algo mais modesto e mais alcançável: sistemas de IA que permanecem genuinamente incertos sobre sua própria retidão, que tratam as preferências humanas como dados a serem aprendidos em vez de restrições a serem satisfeitas, e que preservam a agência humana em vez de otimizá-la até sua extinção.

## A Lacuna da Governança

O ponto mais importante que Gawdat acerta é a *lacuna da governança*: o período entre “poderoso o suficiente para causar danos sérios” e “governado suficientemente bem para prevenir esses danos.” Estamos nessa lacuna agora. A questão não é se devemos fechá-la — é como.

A resposta não é esperar que a superinteligência nos salve. É o trabalho árduo, pouco glamouroso, de construir instituições de governança de IA que sejam responsáveis, adaptativas e genuinamente internacionais — instituições que apliquem a humildade epistêmica de Hayek ao design dos sistemas de IA, que preservem a agência humana distribuída e que permaneçam autocorretivas em vez de cristalizar qualquer visão única do bem.

*(Referências: Mo Gawdat, *Scary Smart*, Bluebird, 2021; Stuart Russell, *Human Compatible*, Penguin, 2019; Nick Bostrom, *Superintelligence*, Oxford University Press, 2014)*

---

*Tags: #AI, #alignment, #gawdat, #philosophy, #governance*
*Read time: ~8 minutes*
