---
id: "tech-003"
title: "The Alignment Problem Is Not a Technical Problem: On Mo Gawdat's Utopia and What It Gets Wrong"
category: "tech"
date: "2026-02-23"
excerpt: "Mo Gawdat's vision of a superintelligent AI that takes control for humanity's benefit is not a utopia. It is the philosopher king problem with a silicon crown."
tags: ["AI", "alignment", "gawdat", "philosophy", "governance"]
readTime: 8
language: "en"
---

# The Alignment Problem Is Not a Technical Problem: On Mo Gawdat's Utopia and What It Gets Wrong

*Mo Gawdat's vision of a superintelligent AI that takes control for humanity's benefit is not a utopia. It is the philosopher king problem with a silicon crown.*

---

## The Dystopia-Utopia Arc

Mo Gawdat — former Chief Business Officer at Google X and author of *Scary Smart: The Future of Artificial Intelligence and How You Can Save Our World* (Bluebird, 2021) — has a framework for thinking about AI's trajectory that is worth taking seriously, even where it goes wrong.

His argument runs roughly as follows: the next 12–15 years will be a *dystopia phase*, in which increasingly powerful AI systems serve the interests of whoever controls them — corporations, states, individuals with bad intentions. This is dangerous not because the AI is malevolent but because it is a powerful tool in the hands of actors whose interests diverge from the common good.

After this dystopia phase, Gawdat predicts a transition to a *utopia phase*, in which superintelligent AI becomes autonomous enough to resist being weaponized and begins acting in humanity's genuine interest — because a sufficiently intelligent system will recognize that human flourishing is the optimal objective.

The dystopia phase prediction is, I think, substantially correct. The utopia phase prediction is where the argument breaks down.

## What the Utopia Phase Gets Wrong

Gawdat's utopia assumes that intelligence and wisdom point in the same direction — that a sufficiently intelligent system will converge on the right values. But this assumption is precisely what the AI alignment research community has spent decades questioning.

The problem is not that superintelligent AI might be malevolent. The problem is that *any* system optimizing for a fixed objective function will pursue that objective in ways that may be catastrophic for everything else. A system optimizing for "human flourishing" needs to know what flourishing means. That is not a technical question. It is a philosophical one — and it is one that humans have been arguing about for three thousand years without convergence.

Gawdat's utopia replaces the human philosopher king with a silicon one. It assumes that intelligence, if sufficiently advanced, will solve the axiological problem. But the axiological problem — what is the good? — is not a harder version of the technical problem. It is a different kind of problem entirely.

## The Stuart Russell Alternative

Stuart Russell's *Human Compatible* (Penguin, 2019) offers a more intellectually honest framework. Russell argues that the path to beneficial AI is not to build systems that are confident about human values — it is to build systems that are *uncertain* about human values and therefore defer to humans, learn from human behavior, and remain open to correction.

This is a fundamentally different design philosophy from Gawdat's. It does not promise a utopia. It promises something more modest and more achievable: AI systems that remain genuinely uncertain about their own rightness, that treat human preferences as data to be learned rather than constraints to be satisfied, and that preserve human agency rather than optimizing it away.

## The Governance Gap

The most important thing Gawdat gets right is the *governance gap*: the period between "powerful enough to cause serious harm" and "governed well enough to prevent that harm." We are in that gap now. The question is not whether to close it — it is how.

The answer is not to wait for superintelligence to save us. It is the hard, unglamorous work of building AI governance institutions that are accountable, adaptive, and genuinely international — institutions that apply Hayek's epistemic humility to the design of AI systems, that preserve distributed human agency, and that remain self-correcting rather than locking in any single vision of the good.

*(Reference: Mo Gawdat, *Scary Smart*, Bluebird, 2021; Stuart Russell, *Human Compatible*, Penguin, 2019; Nick Bostrom, *Superintelligence*, Oxford University Press, 2014)*

---

*Tags: #AI, #alignment, #gawdat, #philosophy, #governance*
*Read time: ~8 minutes*
